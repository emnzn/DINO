{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from utils import (\n",
    "    get_args, \n",
    "    get_model,\n",
    "    save_args,\n",
    "    get_base_lr,\n",
    "    get_dataset,\n",
    "    get_param_groups,\n",
    "    cosine_scheduling,\n",
    "    get_teacher_temperatures,\n",
    "    DINO,\n",
    "    Encoder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Identity()\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(\"vit-s-16\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_args(run_dir: str):\n",
    "    args = get_args(run_dir)\n",
    "\n",
    "    encoder_keys = {\"backbone\", \"mlp_layers\", \"hidden_dim\", \"bottleneck_dim\", \"k_dim\"}\n",
    "\n",
    "    encoder_args = {k: v for k, v in args.items() if k in encoder_keys}\n",
    "\n",
    "    return encoder_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = \"vit-s-16\"\n",
    "experiment_num = 0\n",
    "\n",
    "ckpt_dir = os.path.join(\"..\", \"assets\", \"model-weights\", backbone, \"pre-train\", f\"version_{experiment_num}\", \"min-loss.ckpt\")\n",
    "run_dir = os.path.join(\"..\", \"src\", \"pre-train-runs\", backbone, f\"version_{experiment_num}\", \"run-config.yaml\")\n",
    "\n",
    "ckpt = torch.load(ckpt_dir, map_location=torch.device(\"cpu\"))[\"state_dict\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (encoder): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Identity()\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (mlp): ProjectionHead(\n",
       "    (0): Linear(in_features=384, out_features=2048, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "    )\n",
       "    (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  )\n",
       "  (k_projection): ParametrizedLinear(\n",
       "    in_features=256, out_features=65536, bias=False\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): _WeightNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_args = get_encoder_args(run_dir)\n",
    "\n",
    "encoder = Encoder(**encoder_args)\n",
    "\n",
    "encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.cls_token\n",
      "encoder.pos_embed\n",
      "encoder.patch_embed.proj.weight\n",
      "encoder.patch_embed.proj.bias\n",
      "encoder.blocks.0.norm1.weight\n",
      "encoder.blocks.0.norm1.bias\n",
      "encoder.blocks.0.attn.qkv.weight\n",
      "encoder.blocks.0.attn.qkv.bias\n",
      "encoder.blocks.0.attn.proj.weight\n",
      "encoder.blocks.0.attn.proj.bias\n",
      "encoder.blocks.0.norm2.weight\n",
      "encoder.blocks.0.norm2.bias\n",
      "encoder.blocks.0.mlp.fc1.weight\n",
      "encoder.blocks.0.mlp.fc1.bias\n",
      "encoder.blocks.0.mlp.fc2.weight\n",
      "encoder.blocks.0.mlp.fc2.bias\n",
      "encoder.blocks.1.norm1.weight\n",
      "encoder.blocks.1.norm1.bias\n",
      "encoder.blocks.1.attn.qkv.weight\n",
      "encoder.blocks.1.attn.qkv.bias\n",
      "encoder.blocks.1.attn.proj.weight\n",
      "encoder.blocks.1.attn.proj.bias\n",
      "encoder.blocks.1.norm2.weight\n",
      "encoder.blocks.1.norm2.bias\n",
      "encoder.blocks.1.mlp.fc1.weight\n",
      "encoder.blocks.1.mlp.fc1.bias\n",
      "encoder.blocks.1.mlp.fc2.weight\n",
      "encoder.blocks.1.mlp.fc2.bias\n",
      "encoder.blocks.2.norm1.weight\n",
      "encoder.blocks.2.norm1.bias\n",
      "encoder.blocks.2.attn.qkv.weight\n",
      "encoder.blocks.2.attn.qkv.bias\n",
      "encoder.blocks.2.attn.proj.weight\n",
      "encoder.blocks.2.attn.proj.bias\n",
      "encoder.blocks.2.norm2.weight\n",
      "encoder.blocks.2.norm2.bias\n",
      "encoder.blocks.2.mlp.fc1.weight\n",
      "encoder.blocks.2.mlp.fc1.bias\n",
      "encoder.blocks.2.mlp.fc2.weight\n",
      "encoder.blocks.2.mlp.fc2.bias\n",
      "encoder.blocks.3.norm1.weight\n",
      "encoder.blocks.3.norm1.bias\n",
      "encoder.blocks.3.attn.qkv.weight\n",
      "encoder.blocks.3.attn.qkv.bias\n",
      "encoder.blocks.3.attn.proj.weight\n",
      "encoder.blocks.3.attn.proj.bias\n",
      "encoder.blocks.3.norm2.weight\n",
      "encoder.blocks.3.norm2.bias\n",
      "encoder.blocks.3.mlp.fc1.weight\n",
      "encoder.blocks.3.mlp.fc1.bias\n",
      "encoder.blocks.3.mlp.fc2.weight\n",
      "encoder.blocks.3.mlp.fc2.bias\n",
      "encoder.blocks.4.norm1.weight\n",
      "encoder.blocks.4.norm1.bias\n",
      "encoder.blocks.4.attn.qkv.weight\n",
      "encoder.blocks.4.attn.qkv.bias\n",
      "encoder.blocks.4.attn.proj.weight\n",
      "encoder.blocks.4.attn.proj.bias\n",
      "encoder.blocks.4.norm2.weight\n",
      "encoder.blocks.4.norm2.bias\n",
      "encoder.blocks.4.mlp.fc1.weight\n",
      "encoder.blocks.4.mlp.fc1.bias\n",
      "encoder.blocks.4.mlp.fc2.weight\n",
      "encoder.blocks.4.mlp.fc2.bias\n",
      "encoder.blocks.5.norm1.weight\n",
      "encoder.blocks.5.norm1.bias\n",
      "encoder.blocks.5.attn.qkv.weight\n",
      "encoder.blocks.5.attn.qkv.bias\n",
      "encoder.blocks.5.attn.proj.weight\n",
      "encoder.blocks.5.attn.proj.bias\n",
      "encoder.blocks.5.norm2.weight\n",
      "encoder.blocks.5.norm2.bias\n",
      "encoder.blocks.5.mlp.fc1.weight\n",
      "encoder.blocks.5.mlp.fc1.bias\n",
      "encoder.blocks.5.mlp.fc2.weight\n",
      "encoder.blocks.5.mlp.fc2.bias\n",
      "encoder.blocks.6.norm1.weight\n",
      "encoder.blocks.6.norm1.bias\n",
      "encoder.blocks.6.attn.qkv.weight\n",
      "encoder.blocks.6.attn.qkv.bias\n",
      "encoder.blocks.6.attn.proj.weight\n",
      "encoder.blocks.6.attn.proj.bias\n",
      "encoder.blocks.6.norm2.weight\n",
      "encoder.blocks.6.norm2.bias\n",
      "encoder.blocks.6.mlp.fc1.weight\n",
      "encoder.blocks.6.mlp.fc1.bias\n",
      "encoder.blocks.6.mlp.fc2.weight\n",
      "encoder.blocks.6.mlp.fc2.bias\n",
      "encoder.blocks.7.norm1.weight\n",
      "encoder.blocks.7.norm1.bias\n",
      "encoder.blocks.7.attn.qkv.weight\n",
      "encoder.blocks.7.attn.qkv.bias\n",
      "encoder.blocks.7.attn.proj.weight\n",
      "encoder.blocks.7.attn.proj.bias\n",
      "encoder.blocks.7.norm2.weight\n",
      "encoder.blocks.7.norm2.bias\n",
      "encoder.blocks.7.mlp.fc1.weight\n",
      "encoder.blocks.7.mlp.fc1.bias\n",
      "encoder.blocks.7.mlp.fc2.weight\n",
      "encoder.blocks.7.mlp.fc2.bias\n",
      "encoder.blocks.8.norm1.weight\n",
      "encoder.blocks.8.norm1.bias\n",
      "encoder.blocks.8.attn.qkv.weight\n",
      "encoder.blocks.8.attn.qkv.bias\n",
      "encoder.blocks.8.attn.proj.weight\n",
      "encoder.blocks.8.attn.proj.bias\n",
      "encoder.blocks.8.norm2.weight\n",
      "encoder.blocks.8.norm2.bias\n",
      "encoder.blocks.8.mlp.fc1.weight\n",
      "encoder.blocks.8.mlp.fc1.bias\n",
      "encoder.blocks.8.mlp.fc2.weight\n",
      "encoder.blocks.8.mlp.fc2.bias\n",
      "encoder.blocks.9.norm1.weight\n",
      "encoder.blocks.9.norm1.bias\n",
      "encoder.blocks.9.attn.qkv.weight\n",
      "encoder.blocks.9.attn.qkv.bias\n",
      "encoder.blocks.9.attn.proj.weight\n",
      "encoder.blocks.9.attn.proj.bias\n",
      "encoder.blocks.9.norm2.weight\n",
      "encoder.blocks.9.norm2.bias\n",
      "encoder.blocks.9.mlp.fc1.weight\n",
      "encoder.blocks.9.mlp.fc1.bias\n",
      "encoder.blocks.9.mlp.fc2.weight\n",
      "encoder.blocks.9.mlp.fc2.bias\n",
      "encoder.blocks.10.norm1.weight\n",
      "encoder.blocks.10.norm1.bias\n",
      "encoder.blocks.10.attn.qkv.weight\n",
      "encoder.blocks.10.attn.qkv.bias\n",
      "encoder.blocks.10.attn.proj.weight\n",
      "encoder.blocks.10.attn.proj.bias\n",
      "encoder.blocks.10.norm2.weight\n",
      "encoder.blocks.10.norm2.bias\n",
      "encoder.blocks.10.mlp.fc1.weight\n",
      "encoder.blocks.10.mlp.fc1.bias\n",
      "encoder.blocks.10.mlp.fc2.weight\n",
      "encoder.blocks.10.mlp.fc2.bias\n",
      "encoder.blocks.11.norm1.weight\n",
      "encoder.blocks.11.norm1.bias\n",
      "encoder.blocks.11.attn.qkv.weight\n",
      "encoder.blocks.11.attn.qkv.bias\n",
      "encoder.blocks.11.attn.proj.weight\n",
      "encoder.blocks.11.attn.proj.bias\n",
      "encoder.blocks.11.norm2.weight\n",
      "encoder.blocks.11.norm2.bias\n",
      "encoder.blocks.11.mlp.fc1.weight\n",
      "encoder.blocks.11.mlp.fc1.bias\n",
      "encoder.blocks.11.mlp.fc2.weight\n",
      "encoder.blocks.11.mlp.fc2.bias\n",
      "encoder.norm.weight\n",
      "encoder.norm.bias\n",
      "mlp.0.weight\n",
      "mlp.0.bias\n",
      "mlp.2.0.weight\n",
      "mlp.2.0.bias\n",
      "mlp.3.weight\n",
      "mlp.3.bias\n",
      "k_projection.parametrizations.weight.original0\n",
      "k_projection.parametrizations.weight.original1\n"
     ]
    }
   ],
   "source": [
    "for name, _ in encoder.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student.encoder.cls_token\n",
      "student.encoder.pos_embed\n",
      "student.encoder.patch_embed.proj.weight\n",
      "student.encoder.patch_embed.proj.bias\n",
      "student.encoder.blocks.0.norm1.weight\n",
      "student.encoder.blocks.0.norm1.bias\n",
      "student.encoder.blocks.0.attn.qkv.weight\n",
      "student.encoder.blocks.0.attn.qkv.bias\n",
      "student.encoder.blocks.0.attn.proj.weight\n",
      "student.encoder.blocks.0.attn.proj.bias\n",
      "student.encoder.blocks.0.norm2.weight\n",
      "student.encoder.blocks.0.norm2.bias\n",
      "student.encoder.blocks.0.mlp.fc1.weight\n",
      "student.encoder.blocks.0.mlp.fc1.bias\n",
      "student.encoder.blocks.0.mlp.fc2.weight\n",
      "student.encoder.blocks.0.mlp.fc2.bias\n",
      "student.encoder.blocks.1.norm1.weight\n",
      "student.encoder.blocks.1.norm1.bias\n",
      "student.encoder.blocks.1.attn.qkv.weight\n",
      "student.encoder.blocks.1.attn.qkv.bias\n",
      "student.encoder.blocks.1.attn.proj.weight\n",
      "student.encoder.blocks.1.attn.proj.bias\n",
      "student.encoder.blocks.1.norm2.weight\n",
      "student.encoder.blocks.1.norm2.bias\n",
      "student.encoder.blocks.1.mlp.fc1.weight\n",
      "student.encoder.blocks.1.mlp.fc1.bias\n",
      "student.encoder.blocks.1.mlp.fc2.weight\n",
      "student.encoder.blocks.1.mlp.fc2.bias\n",
      "student.encoder.blocks.2.norm1.weight\n",
      "student.encoder.blocks.2.norm1.bias\n",
      "student.encoder.blocks.2.attn.qkv.weight\n",
      "student.encoder.blocks.2.attn.qkv.bias\n",
      "student.encoder.blocks.2.attn.proj.weight\n",
      "student.encoder.blocks.2.attn.proj.bias\n",
      "student.encoder.blocks.2.norm2.weight\n",
      "student.encoder.blocks.2.norm2.bias\n",
      "student.encoder.blocks.2.mlp.fc1.weight\n",
      "student.encoder.blocks.2.mlp.fc1.bias\n",
      "student.encoder.blocks.2.mlp.fc2.weight\n",
      "student.encoder.blocks.2.mlp.fc2.bias\n",
      "student.encoder.blocks.3.norm1.weight\n",
      "student.encoder.blocks.3.norm1.bias\n",
      "student.encoder.blocks.3.attn.qkv.weight\n",
      "student.encoder.blocks.3.attn.qkv.bias\n",
      "student.encoder.blocks.3.attn.proj.weight\n",
      "student.encoder.blocks.3.attn.proj.bias\n",
      "student.encoder.blocks.3.norm2.weight\n",
      "student.encoder.blocks.3.norm2.bias\n",
      "student.encoder.blocks.3.mlp.fc1.weight\n",
      "student.encoder.blocks.3.mlp.fc1.bias\n",
      "student.encoder.blocks.3.mlp.fc2.weight\n",
      "student.encoder.blocks.3.mlp.fc2.bias\n",
      "student.encoder.blocks.4.norm1.weight\n",
      "student.encoder.blocks.4.norm1.bias\n",
      "student.encoder.blocks.4.attn.qkv.weight\n",
      "student.encoder.blocks.4.attn.qkv.bias\n",
      "student.encoder.blocks.4.attn.proj.weight\n",
      "student.encoder.blocks.4.attn.proj.bias\n",
      "student.encoder.blocks.4.norm2.weight\n",
      "student.encoder.blocks.4.norm2.bias\n",
      "student.encoder.blocks.4.mlp.fc1.weight\n",
      "student.encoder.blocks.4.mlp.fc1.bias\n",
      "student.encoder.blocks.4.mlp.fc2.weight\n",
      "student.encoder.blocks.4.mlp.fc2.bias\n",
      "student.encoder.blocks.5.norm1.weight\n",
      "student.encoder.blocks.5.norm1.bias\n",
      "student.encoder.blocks.5.attn.qkv.weight\n",
      "student.encoder.blocks.5.attn.qkv.bias\n",
      "student.encoder.blocks.5.attn.proj.weight\n",
      "student.encoder.blocks.5.attn.proj.bias\n",
      "student.encoder.blocks.5.norm2.weight\n",
      "student.encoder.blocks.5.norm2.bias\n",
      "student.encoder.blocks.5.mlp.fc1.weight\n",
      "student.encoder.blocks.5.mlp.fc1.bias\n",
      "student.encoder.blocks.5.mlp.fc2.weight\n",
      "student.encoder.blocks.5.mlp.fc2.bias\n",
      "student.encoder.blocks.6.norm1.weight\n",
      "student.encoder.blocks.6.norm1.bias\n",
      "student.encoder.blocks.6.attn.qkv.weight\n",
      "student.encoder.blocks.6.attn.qkv.bias\n",
      "student.encoder.blocks.6.attn.proj.weight\n",
      "student.encoder.blocks.6.attn.proj.bias\n",
      "student.encoder.blocks.6.norm2.weight\n",
      "student.encoder.blocks.6.norm2.bias\n",
      "student.encoder.blocks.6.mlp.fc1.weight\n",
      "student.encoder.blocks.6.mlp.fc1.bias\n",
      "student.encoder.blocks.6.mlp.fc2.weight\n",
      "student.encoder.blocks.6.mlp.fc2.bias\n",
      "student.encoder.blocks.7.norm1.weight\n",
      "student.encoder.blocks.7.norm1.bias\n",
      "student.encoder.blocks.7.attn.qkv.weight\n",
      "student.encoder.blocks.7.attn.qkv.bias\n",
      "student.encoder.blocks.7.attn.proj.weight\n",
      "student.encoder.blocks.7.attn.proj.bias\n",
      "student.encoder.blocks.7.norm2.weight\n",
      "student.encoder.blocks.7.norm2.bias\n",
      "student.encoder.blocks.7.mlp.fc1.weight\n",
      "student.encoder.blocks.7.mlp.fc1.bias\n",
      "student.encoder.blocks.7.mlp.fc2.weight\n",
      "student.encoder.blocks.7.mlp.fc2.bias\n",
      "student.encoder.blocks.8.norm1.weight\n",
      "student.encoder.blocks.8.norm1.bias\n",
      "student.encoder.blocks.8.attn.qkv.weight\n",
      "student.encoder.blocks.8.attn.qkv.bias\n",
      "student.encoder.blocks.8.attn.proj.weight\n",
      "student.encoder.blocks.8.attn.proj.bias\n",
      "student.encoder.blocks.8.norm2.weight\n",
      "student.encoder.blocks.8.norm2.bias\n",
      "student.encoder.blocks.8.mlp.fc1.weight\n",
      "student.encoder.blocks.8.mlp.fc1.bias\n",
      "student.encoder.blocks.8.mlp.fc2.weight\n",
      "student.encoder.blocks.8.mlp.fc2.bias\n",
      "student.encoder.blocks.9.norm1.weight\n",
      "student.encoder.blocks.9.norm1.bias\n",
      "student.encoder.blocks.9.attn.qkv.weight\n",
      "student.encoder.blocks.9.attn.qkv.bias\n",
      "student.encoder.blocks.9.attn.proj.weight\n",
      "student.encoder.blocks.9.attn.proj.bias\n",
      "student.encoder.blocks.9.norm2.weight\n",
      "student.encoder.blocks.9.norm2.bias\n",
      "student.encoder.blocks.9.mlp.fc1.weight\n",
      "student.encoder.blocks.9.mlp.fc1.bias\n",
      "student.encoder.blocks.9.mlp.fc2.weight\n",
      "student.encoder.blocks.9.mlp.fc2.bias\n",
      "student.encoder.blocks.10.norm1.weight\n",
      "student.encoder.blocks.10.norm1.bias\n",
      "student.encoder.blocks.10.attn.qkv.weight\n",
      "student.encoder.blocks.10.attn.qkv.bias\n",
      "student.encoder.blocks.10.attn.proj.weight\n",
      "student.encoder.blocks.10.attn.proj.bias\n",
      "student.encoder.blocks.10.norm2.weight\n",
      "student.encoder.blocks.10.norm2.bias\n",
      "student.encoder.blocks.10.mlp.fc1.weight\n",
      "student.encoder.blocks.10.mlp.fc1.bias\n",
      "student.encoder.blocks.10.mlp.fc2.weight\n",
      "student.encoder.blocks.10.mlp.fc2.bias\n",
      "student.encoder.blocks.11.norm1.weight\n",
      "student.encoder.blocks.11.norm1.bias\n",
      "student.encoder.blocks.11.attn.qkv.weight\n",
      "student.encoder.blocks.11.attn.qkv.bias\n",
      "student.encoder.blocks.11.attn.proj.weight\n",
      "student.encoder.blocks.11.attn.proj.bias\n",
      "student.encoder.blocks.11.norm2.weight\n",
      "student.encoder.blocks.11.norm2.bias\n",
      "student.encoder.blocks.11.mlp.fc1.weight\n",
      "student.encoder.blocks.11.mlp.fc1.bias\n",
      "student.encoder.blocks.11.mlp.fc2.weight\n",
      "student.encoder.blocks.11.mlp.fc2.bias\n",
      "student.encoder.norm.weight\n",
      "student.encoder.norm.bias\n",
      "student.mlp.0.weight\n",
      "student.mlp.0.bias\n",
      "student.mlp.2.0.weight\n",
      "student.mlp.2.0.bias\n",
      "student.mlp.3.weight\n",
      "student.mlp.3.bias\n",
      "student.k_projection.parametrizations.weight.original0\n",
      "student.k_projection.parametrizations.weight.original1\n",
      "teacher.encoder.cls_token\n",
      "teacher.encoder.pos_embed\n",
      "teacher.encoder.patch_embed.proj.weight\n",
      "teacher.encoder.patch_embed.proj.bias\n",
      "teacher.encoder.blocks.0.norm1.weight\n",
      "teacher.encoder.blocks.0.norm1.bias\n",
      "teacher.encoder.blocks.0.attn.qkv.weight\n",
      "teacher.encoder.blocks.0.attn.qkv.bias\n",
      "teacher.encoder.blocks.0.attn.proj.weight\n",
      "teacher.encoder.blocks.0.attn.proj.bias\n",
      "teacher.encoder.blocks.0.norm2.weight\n",
      "teacher.encoder.blocks.0.norm2.bias\n",
      "teacher.encoder.blocks.0.mlp.fc1.weight\n",
      "teacher.encoder.blocks.0.mlp.fc1.bias\n",
      "teacher.encoder.blocks.0.mlp.fc2.weight\n",
      "teacher.encoder.blocks.0.mlp.fc2.bias\n",
      "teacher.encoder.blocks.1.norm1.weight\n",
      "teacher.encoder.blocks.1.norm1.bias\n",
      "teacher.encoder.blocks.1.attn.qkv.weight\n",
      "teacher.encoder.blocks.1.attn.qkv.bias\n",
      "teacher.encoder.blocks.1.attn.proj.weight\n",
      "teacher.encoder.blocks.1.attn.proj.bias\n",
      "teacher.encoder.blocks.1.norm2.weight\n",
      "teacher.encoder.blocks.1.norm2.bias\n",
      "teacher.encoder.blocks.1.mlp.fc1.weight\n",
      "teacher.encoder.blocks.1.mlp.fc1.bias\n",
      "teacher.encoder.blocks.1.mlp.fc2.weight\n",
      "teacher.encoder.blocks.1.mlp.fc2.bias\n",
      "teacher.encoder.blocks.2.norm1.weight\n",
      "teacher.encoder.blocks.2.norm1.bias\n",
      "teacher.encoder.blocks.2.attn.qkv.weight\n",
      "teacher.encoder.blocks.2.attn.qkv.bias\n",
      "teacher.encoder.blocks.2.attn.proj.weight\n",
      "teacher.encoder.blocks.2.attn.proj.bias\n",
      "teacher.encoder.blocks.2.norm2.weight\n",
      "teacher.encoder.blocks.2.norm2.bias\n",
      "teacher.encoder.blocks.2.mlp.fc1.weight\n",
      "teacher.encoder.blocks.2.mlp.fc1.bias\n",
      "teacher.encoder.blocks.2.mlp.fc2.weight\n",
      "teacher.encoder.blocks.2.mlp.fc2.bias\n",
      "teacher.encoder.blocks.3.norm1.weight\n",
      "teacher.encoder.blocks.3.norm1.bias\n",
      "teacher.encoder.blocks.3.attn.qkv.weight\n",
      "teacher.encoder.blocks.3.attn.qkv.bias\n",
      "teacher.encoder.blocks.3.attn.proj.weight\n",
      "teacher.encoder.blocks.3.attn.proj.bias\n",
      "teacher.encoder.blocks.3.norm2.weight\n",
      "teacher.encoder.blocks.3.norm2.bias\n",
      "teacher.encoder.blocks.3.mlp.fc1.weight\n",
      "teacher.encoder.blocks.3.mlp.fc1.bias\n",
      "teacher.encoder.blocks.3.mlp.fc2.weight\n",
      "teacher.encoder.blocks.3.mlp.fc2.bias\n",
      "teacher.encoder.blocks.4.norm1.weight\n",
      "teacher.encoder.blocks.4.norm1.bias\n",
      "teacher.encoder.blocks.4.attn.qkv.weight\n",
      "teacher.encoder.blocks.4.attn.qkv.bias\n",
      "teacher.encoder.blocks.4.attn.proj.weight\n",
      "teacher.encoder.blocks.4.attn.proj.bias\n",
      "teacher.encoder.blocks.4.norm2.weight\n",
      "teacher.encoder.blocks.4.norm2.bias\n",
      "teacher.encoder.blocks.4.mlp.fc1.weight\n",
      "teacher.encoder.blocks.4.mlp.fc1.bias\n",
      "teacher.encoder.blocks.4.mlp.fc2.weight\n",
      "teacher.encoder.blocks.4.mlp.fc2.bias\n",
      "teacher.encoder.blocks.5.norm1.weight\n",
      "teacher.encoder.blocks.5.norm1.bias\n",
      "teacher.encoder.blocks.5.attn.qkv.weight\n",
      "teacher.encoder.blocks.5.attn.qkv.bias\n",
      "teacher.encoder.blocks.5.attn.proj.weight\n",
      "teacher.encoder.blocks.5.attn.proj.bias\n",
      "teacher.encoder.blocks.5.norm2.weight\n",
      "teacher.encoder.blocks.5.norm2.bias\n",
      "teacher.encoder.blocks.5.mlp.fc1.weight\n",
      "teacher.encoder.blocks.5.mlp.fc1.bias\n",
      "teacher.encoder.blocks.5.mlp.fc2.weight\n",
      "teacher.encoder.blocks.5.mlp.fc2.bias\n",
      "teacher.encoder.blocks.6.norm1.weight\n",
      "teacher.encoder.blocks.6.norm1.bias\n",
      "teacher.encoder.blocks.6.attn.qkv.weight\n",
      "teacher.encoder.blocks.6.attn.qkv.bias\n",
      "teacher.encoder.blocks.6.attn.proj.weight\n",
      "teacher.encoder.blocks.6.attn.proj.bias\n",
      "teacher.encoder.blocks.6.norm2.weight\n",
      "teacher.encoder.blocks.6.norm2.bias\n",
      "teacher.encoder.blocks.6.mlp.fc1.weight\n",
      "teacher.encoder.blocks.6.mlp.fc1.bias\n",
      "teacher.encoder.blocks.6.mlp.fc2.weight\n",
      "teacher.encoder.blocks.6.mlp.fc2.bias\n",
      "teacher.encoder.blocks.7.norm1.weight\n",
      "teacher.encoder.blocks.7.norm1.bias\n",
      "teacher.encoder.blocks.7.attn.qkv.weight\n",
      "teacher.encoder.blocks.7.attn.qkv.bias\n",
      "teacher.encoder.blocks.7.attn.proj.weight\n",
      "teacher.encoder.blocks.7.attn.proj.bias\n",
      "teacher.encoder.blocks.7.norm2.weight\n",
      "teacher.encoder.blocks.7.norm2.bias\n",
      "teacher.encoder.blocks.7.mlp.fc1.weight\n",
      "teacher.encoder.blocks.7.mlp.fc1.bias\n",
      "teacher.encoder.blocks.7.mlp.fc2.weight\n",
      "teacher.encoder.blocks.7.mlp.fc2.bias\n",
      "teacher.encoder.blocks.8.norm1.weight\n",
      "teacher.encoder.blocks.8.norm1.bias\n",
      "teacher.encoder.blocks.8.attn.qkv.weight\n",
      "teacher.encoder.blocks.8.attn.qkv.bias\n",
      "teacher.encoder.blocks.8.attn.proj.weight\n",
      "teacher.encoder.blocks.8.attn.proj.bias\n",
      "teacher.encoder.blocks.8.norm2.weight\n",
      "teacher.encoder.blocks.8.norm2.bias\n",
      "teacher.encoder.blocks.8.mlp.fc1.weight\n",
      "teacher.encoder.blocks.8.mlp.fc1.bias\n",
      "teacher.encoder.blocks.8.mlp.fc2.weight\n",
      "teacher.encoder.blocks.8.mlp.fc2.bias\n",
      "teacher.encoder.blocks.9.norm1.weight\n",
      "teacher.encoder.blocks.9.norm1.bias\n",
      "teacher.encoder.blocks.9.attn.qkv.weight\n",
      "teacher.encoder.blocks.9.attn.qkv.bias\n",
      "teacher.encoder.blocks.9.attn.proj.weight\n",
      "teacher.encoder.blocks.9.attn.proj.bias\n",
      "teacher.encoder.blocks.9.norm2.weight\n",
      "teacher.encoder.blocks.9.norm2.bias\n",
      "teacher.encoder.blocks.9.mlp.fc1.weight\n",
      "teacher.encoder.blocks.9.mlp.fc1.bias\n",
      "teacher.encoder.blocks.9.mlp.fc2.weight\n",
      "teacher.encoder.blocks.9.mlp.fc2.bias\n",
      "teacher.encoder.blocks.10.norm1.weight\n",
      "teacher.encoder.blocks.10.norm1.bias\n",
      "teacher.encoder.blocks.10.attn.qkv.weight\n",
      "teacher.encoder.blocks.10.attn.qkv.bias\n",
      "teacher.encoder.blocks.10.attn.proj.weight\n",
      "teacher.encoder.blocks.10.attn.proj.bias\n",
      "teacher.encoder.blocks.10.norm2.weight\n",
      "teacher.encoder.blocks.10.norm2.bias\n",
      "teacher.encoder.blocks.10.mlp.fc1.weight\n",
      "teacher.encoder.blocks.10.mlp.fc1.bias\n",
      "teacher.encoder.blocks.10.mlp.fc2.weight\n",
      "teacher.encoder.blocks.10.mlp.fc2.bias\n",
      "teacher.encoder.blocks.11.norm1.weight\n",
      "teacher.encoder.blocks.11.norm1.bias\n",
      "teacher.encoder.blocks.11.attn.qkv.weight\n",
      "teacher.encoder.blocks.11.attn.qkv.bias\n",
      "teacher.encoder.blocks.11.attn.proj.weight\n",
      "teacher.encoder.blocks.11.attn.proj.bias\n",
      "teacher.encoder.blocks.11.norm2.weight\n",
      "teacher.encoder.blocks.11.norm2.bias\n",
      "teacher.encoder.blocks.11.mlp.fc1.weight\n",
      "teacher.encoder.blocks.11.mlp.fc1.bias\n",
      "teacher.encoder.blocks.11.mlp.fc2.weight\n",
      "teacher.encoder.blocks.11.mlp.fc2.bias\n",
      "teacher.encoder.norm.weight\n",
      "teacher.encoder.norm.bias\n",
      "teacher.mlp.0.weight\n",
      "teacher.mlp.0.bias\n",
      "teacher.mlp.2.0.weight\n",
      "teacher.mlp.2.0.bias\n",
      "teacher.mlp.3.weight\n",
      "teacher.mlp.3.bias\n",
      "teacher.k_projection.parametrizations.weight.original0\n",
      "teacher.k_projection.parametrizations.weight.original1\n"
     ]
    }
   ],
   "source": [
    "for i in ckpt.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['student.encoder.cls_token', 'student.encoder.pos_embed', 'student.encoder.patch_embed.proj.weight', 'student.encoder.patch_embed.proj.bias', 'student.encoder.blocks.0.norm1.weight', 'student.encoder.blocks.0.norm1.bias', 'student.encoder.blocks.0.attn.qkv.weight', 'student.encoder.blocks.0.attn.qkv.bias', 'student.encoder.blocks.0.attn.proj.weight', 'student.encoder.blocks.0.attn.proj.bias', 'student.encoder.blocks.0.norm2.weight', 'student.encoder.blocks.0.norm2.bias', 'student.encoder.blocks.0.mlp.fc1.weight', 'student.encoder.blocks.0.mlp.fc1.bias', 'student.encoder.blocks.0.mlp.fc2.weight', 'student.encoder.blocks.0.mlp.fc2.bias', 'student.encoder.blocks.1.norm1.weight', 'student.encoder.blocks.1.norm1.bias', 'student.encoder.blocks.1.attn.qkv.weight', 'student.encoder.blocks.1.attn.qkv.bias', 'student.encoder.blocks.1.attn.proj.weight', 'student.encoder.blocks.1.attn.proj.bias', 'student.encoder.blocks.1.norm2.weight', 'student.encoder.blocks.1.norm2.bias', 'student.encoder.blocks.1.mlp.fc1.weight', 'student.encoder.blocks.1.mlp.fc1.bias', 'student.encoder.blocks.1.mlp.fc2.weight', 'student.encoder.blocks.1.mlp.fc2.bias', 'student.encoder.blocks.2.norm1.weight', 'student.encoder.blocks.2.norm1.bias', 'student.encoder.blocks.2.attn.qkv.weight', 'student.encoder.blocks.2.attn.qkv.bias', 'student.encoder.blocks.2.attn.proj.weight', 'student.encoder.blocks.2.attn.proj.bias', 'student.encoder.blocks.2.norm2.weight', 'student.encoder.blocks.2.norm2.bias', 'student.encoder.blocks.2.mlp.fc1.weight', 'student.encoder.blocks.2.mlp.fc1.bias', 'student.encoder.blocks.2.mlp.fc2.weight', 'student.encoder.blocks.2.mlp.fc2.bias', 'student.encoder.blocks.3.norm1.weight', 'student.encoder.blocks.3.norm1.bias', 'student.encoder.blocks.3.attn.qkv.weight', 'student.encoder.blocks.3.attn.qkv.bias', 'student.encoder.blocks.3.attn.proj.weight', 'student.encoder.blocks.3.attn.proj.bias', 'student.encoder.blocks.3.norm2.weight', 'student.encoder.blocks.3.norm2.bias', 'student.encoder.blocks.3.mlp.fc1.weight', 'student.encoder.blocks.3.mlp.fc1.bias', 'student.encoder.blocks.3.mlp.fc2.weight', 'student.encoder.blocks.3.mlp.fc2.bias', 'student.encoder.blocks.4.norm1.weight', 'student.encoder.blocks.4.norm1.bias', 'student.encoder.blocks.4.attn.qkv.weight', 'student.encoder.blocks.4.attn.qkv.bias', 'student.encoder.blocks.4.attn.proj.weight', 'student.encoder.blocks.4.attn.proj.bias', 'student.encoder.blocks.4.norm2.weight', 'student.encoder.blocks.4.norm2.bias', 'student.encoder.blocks.4.mlp.fc1.weight', 'student.encoder.blocks.4.mlp.fc1.bias', 'student.encoder.blocks.4.mlp.fc2.weight', 'student.encoder.blocks.4.mlp.fc2.bias', 'student.encoder.blocks.5.norm1.weight', 'student.encoder.blocks.5.norm1.bias', 'student.encoder.blocks.5.attn.qkv.weight', 'student.encoder.blocks.5.attn.qkv.bias', 'student.encoder.blocks.5.attn.proj.weight', 'student.encoder.blocks.5.attn.proj.bias', 'student.encoder.blocks.5.norm2.weight', 'student.encoder.blocks.5.norm2.bias', 'student.encoder.blocks.5.mlp.fc1.weight', 'student.encoder.blocks.5.mlp.fc1.bias', 'student.encoder.blocks.5.mlp.fc2.weight', 'student.encoder.blocks.5.mlp.fc2.bias', 'student.encoder.blocks.6.norm1.weight', 'student.encoder.blocks.6.norm1.bias', 'student.encoder.blocks.6.attn.qkv.weight', 'student.encoder.blocks.6.attn.qkv.bias', 'student.encoder.blocks.6.attn.proj.weight', 'student.encoder.blocks.6.attn.proj.bias', 'student.encoder.blocks.6.norm2.weight', 'student.encoder.blocks.6.norm2.bias', 'student.encoder.blocks.6.mlp.fc1.weight', 'student.encoder.blocks.6.mlp.fc1.bias', 'student.encoder.blocks.6.mlp.fc2.weight', 'student.encoder.blocks.6.mlp.fc2.bias', 'student.encoder.blocks.7.norm1.weight', 'student.encoder.blocks.7.norm1.bias', 'student.encoder.blocks.7.attn.qkv.weight', 'student.encoder.blocks.7.attn.qkv.bias', 'student.encoder.blocks.7.attn.proj.weight', 'student.encoder.blocks.7.attn.proj.bias', 'student.encoder.blocks.7.norm2.weight', 'student.encoder.blocks.7.norm2.bias', 'student.encoder.blocks.7.mlp.fc1.weight', 'student.encoder.blocks.7.mlp.fc1.bias', 'student.encoder.blocks.7.mlp.fc2.weight', 'student.encoder.blocks.7.mlp.fc2.bias', 'student.encoder.blocks.8.norm1.weight', 'student.encoder.blocks.8.norm1.bias', 'student.encoder.blocks.8.attn.qkv.weight', 'student.encoder.blocks.8.attn.qkv.bias', 'student.encoder.blocks.8.attn.proj.weight', 'student.encoder.blocks.8.attn.proj.bias', 'student.encoder.blocks.8.norm2.weight', 'student.encoder.blocks.8.norm2.bias', 'student.encoder.blocks.8.mlp.fc1.weight', 'student.encoder.blocks.8.mlp.fc1.bias', 'student.encoder.blocks.8.mlp.fc2.weight', 'student.encoder.blocks.8.mlp.fc2.bias', 'student.encoder.blocks.9.norm1.weight', 'student.encoder.blocks.9.norm1.bias', 'student.encoder.blocks.9.attn.qkv.weight', 'student.encoder.blocks.9.attn.qkv.bias', 'student.encoder.blocks.9.attn.proj.weight', 'student.encoder.blocks.9.attn.proj.bias', 'student.encoder.blocks.9.norm2.weight', 'student.encoder.blocks.9.norm2.bias', 'student.encoder.blocks.9.mlp.fc1.weight', 'student.encoder.blocks.9.mlp.fc1.bias', 'student.encoder.blocks.9.mlp.fc2.weight', 'student.encoder.blocks.9.mlp.fc2.bias', 'student.encoder.blocks.10.norm1.weight', 'student.encoder.blocks.10.norm1.bias', 'student.encoder.blocks.10.attn.qkv.weight', 'student.encoder.blocks.10.attn.qkv.bias', 'student.encoder.blocks.10.attn.proj.weight', 'student.encoder.blocks.10.attn.proj.bias', 'student.encoder.blocks.10.norm2.weight', 'student.encoder.blocks.10.norm2.bias', 'student.encoder.blocks.10.mlp.fc1.weight', 'student.encoder.blocks.10.mlp.fc1.bias', 'student.encoder.blocks.10.mlp.fc2.weight', 'student.encoder.blocks.10.mlp.fc2.bias', 'student.encoder.blocks.11.norm1.weight', 'student.encoder.blocks.11.norm1.bias', 'student.encoder.blocks.11.attn.qkv.weight', 'student.encoder.blocks.11.attn.qkv.bias', 'student.encoder.blocks.11.attn.proj.weight', 'student.encoder.blocks.11.attn.proj.bias', 'student.encoder.blocks.11.norm2.weight', 'student.encoder.blocks.11.norm2.bias', 'student.encoder.blocks.11.mlp.fc1.weight', 'student.encoder.blocks.11.mlp.fc1.bias', 'student.encoder.blocks.11.mlp.fc2.weight', 'student.encoder.blocks.11.mlp.fc2.bias', 'student.encoder.norm.weight', 'student.encoder.norm.bias', 'student.mlp.0.weight', 'student.mlp.0.bias', 'student.mlp.2.0.weight', 'student.mlp.2.0.bias', 'student.mlp.3.weight', 'student.mlp.3.bias', 'student.k_projection.parametrizations.weight.original0', 'student.k_projection.parametrizations.weight.original1'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_params = {k: params for k, params in ckpt.items() if \"student.\" in k}\n",
    "\n",
    "student_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder.cls_token', 'encoder.pos_embed', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.blocks.0.norm1.weight', 'encoder.blocks.0.norm1.bias', 'encoder.blocks.0.attn.qkv.weight', 'encoder.blocks.0.attn.qkv.bias', 'encoder.blocks.0.attn.proj.weight', 'encoder.blocks.0.attn.proj.bias', 'encoder.blocks.0.norm2.weight', 'encoder.blocks.0.norm2.bias', 'encoder.blocks.0.mlp.fc1.weight', 'encoder.blocks.0.mlp.fc1.bias', 'encoder.blocks.0.mlp.fc2.weight', 'encoder.blocks.0.mlp.fc2.bias', 'encoder.blocks.1.norm1.weight', 'encoder.blocks.1.norm1.bias', 'encoder.blocks.1.attn.qkv.weight', 'encoder.blocks.1.attn.qkv.bias', 'encoder.blocks.1.attn.proj.weight', 'encoder.blocks.1.attn.proj.bias', 'encoder.blocks.1.norm2.weight', 'encoder.blocks.1.norm2.bias', 'encoder.blocks.1.mlp.fc1.weight', 'encoder.blocks.1.mlp.fc1.bias', 'encoder.blocks.1.mlp.fc2.weight', 'encoder.blocks.1.mlp.fc2.bias', 'encoder.blocks.2.norm1.weight', 'encoder.blocks.2.norm1.bias', 'encoder.blocks.2.attn.qkv.weight', 'encoder.blocks.2.attn.qkv.bias', 'encoder.blocks.2.attn.proj.weight', 'encoder.blocks.2.attn.proj.bias', 'encoder.blocks.2.norm2.weight', 'encoder.blocks.2.norm2.bias', 'encoder.blocks.2.mlp.fc1.weight', 'encoder.blocks.2.mlp.fc1.bias', 'encoder.blocks.2.mlp.fc2.weight', 'encoder.blocks.2.mlp.fc2.bias', 'encoder.blocks.3.norm1.weight', 'encoder.blocks.3.norm1.bias', 'encoder.blocks.3.attn.qkv.weight', 'encoder.blocks.3.attn.qkv.bias', 'encoder.blocks.3.attn.proj.weight', 'encoder.blocks.3.attn.proj.bias', 'encoder.blocks.3.norm2.weight', 'encoder.blocks.3.norm2.bias', 'encoder.blocks.3.mlp.fc1.weight', 'encoder.blocks.3.mlp.fc1.bias', 'encoder.blocks.3.mlp.fc2.weight', 'encoder.blocks.3.mlp.fc2.bias', 'encoder.blocks.4.norm1.weight', 'encoder.blocks.4.norm1.bias', 'encoder.blocks.4.attn.qkv.weight', 'encoder.blocks.4.attn.qkv.bias', 'encoder.blocks.4.attn.proj.weight', 'encoder.blocks.4.attn.proj.bias', 'encoder.blocks.4.norm2.weight', 'encoder.blocks.4.norm2.bias', 'encoder.blocks.4.mlp.fc1.weight', 'encoder.blocks.4.mlp.fc1.bias', 'encoder.blocks.4.mlp.fc2.weight', 'encoder.blocks.4.mlp.fc2.bias', 'encoder.blocks.5.norm1.weight', 'encoder.blocks.5.norm1.bias', 'encoder.blocks.5.attn.qkv.weight', 'encoder.blocks.5.attn.qkv.bias', 'encoder.blocks.5.attn.proj.weight', 'encoder.blocks.5.attn.proj.bias', 'encoder.blocks.5.norm2.weight', 'encoder.blocks.5.norm2.bias', 'encoder.blocks.5.mlp.fc1.weight', 'encoder.blocks.5.mlp.fc1.bias', 'encoder.blocks.5.mlp.fc2.weight', 'encoder.blocks.5.mlp.fc2.bias', 'encoder.blocks.6.norm1.weight', 'encoder.blocks.6.norm1.bias', 'encoder.blocks.6.attn.qkv.weight', 'encoder.blocks.6.attn.qkv.bias', 'encoder.blocks.6.attn.proj.weight', 'encoder.blocks.6.attn.proj.bias', 'encoder.blocks.6.norm2.weight', 'encoder.blocks.6.norm2.bias', 'encoder.blocks.6.mlp.fc1.weight', 'encoder.blocks.6.mlp.fc1.bias', 'encoder.blocks.6.mlp.fc2.weight', 'encoder.blocks.6.mlp.fc2.bias', 'encoder.blocks.7.norm1.weight', 'encoder.blocks.7.norm1.bias', 'encoder.blocks.7.attn.qkv.weight', 'encoder.blocks.7.attn.qkv.bias', 'encoder.blocks.7.attn.proj.weight', 'encoder.blocks.7.attn.proj.bias', 'encoder.blocks.7.norm2.weight', 'encoder.blocks.7.norm2.bias', 'encoder.blocks.7.mlp.fc1.weight', 'encoder.blocks.7.mlp.fc1.bias', 'encoder.blocks.7.mlp.fc2.weight', 'encoder.blocks.7.mlp.fc2.bias', 'encoder.blocks.8.norm1.weight', 'encoder.blocks.8.norm1.bias', 'encoder.blocks.8.attn.qkv.weight', 'encoder.blocks.8.attn.qkv.bias', 'encoder.blocks.8.attn.proj.weight', 'encoder.blocks.8.attn.proj.bias', 'encoder.blocks.8.norm2.weight', 'encoder.blocks.8.norm2.bias', 'encoder.blocks.8.mlp.fc1.weight', 'encoder.blocks.8.mlp.fc1.bias', 'encoder.blocks.8.mlp.fc2.weight', 'encoder.blocks.8.mlp.fc2.bias', 'encoder.blocks.9.norm1.weight', 'encoder.blocks.9.norm1.bias', 'encoder.blocks.9.attn.qkv.weight', 'encoder.blocks.9.attn.qkv.bias', 'encoder.blocks.9.attn.proj.weight', 'encoder.blocks.9.attn.proj.bias', 'encoder.blocks.9.norm2.weight', 'encoder.blocks.9.norm2.bias', 'encoder.blocks.9.mlp.fc1.weight', 'encoder.blocks.9.mlp.fc1.bias', 'encoder.blocks.9.mlp.fc2.weight', 'encoder.blocks.9.mlp.fc2.bias', 'encoder.blocks.10.norm1.weight', 'encoder.blocks.10.norm1.bias', 'encoder.blocks.10.attn.qkv.weight', 'encoder.blocks.10.attn.qkv.bias', 'encoder.blocks.10.attn.proj.weight', 'encoder.blocks.10.attn.proj.bias', 'encoder.blocks.10.norm2.weight', 'encoder.blocks.10.norm2.bias', 'encoder.blocks.10.mlp.fc1.weight', 'encoder.blocks.10.mlp.fc1.bias', 'encoder.blocks.10.mlp.fc2.weight', 'encoder.blocks.10.mlp.fc2.bias', 'encoder.blocks.11.norm1.weight', 'encoder.blocks.11.norm1.bias', 'encoder.blocks.11.attn.qkv.weight', 'encoder.blocks.11.attn.qkv.bias', 'encoder.blocks.11.attn.proj.weight', 'encoder.blocks.11.attn.proj.bias', 'encoder.blocks.11.norm2.weight', 'encoder.blocks.11.norm2.bias', 'encoder.blocks.11.mlp.fc1.weight', 'encoder.blocks.11.mlp.fc1.bias', 'encoder.blocks.11.mlp.fc2.weight', 'encoder.blocks.11.mlp.fc2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.0.weight', 'mlp.2.0.bias', 'mlp.3.weight', 'mlp.3.bias', 'k_projection.parametrizations.weight.original0', 'k_projection.parametrizations.weight.original1'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_params = {k.replace(\"student.\", \"\"): params for k, params in student_params.items()}\n",
    "\n",
    "student_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(student_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (encoder): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Identity()\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (mlp): ProjectionHead(\n",
       "    (0): Linear(in_features=384, out_features=2048, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "    )\n",
       "    (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  )\n",
       "  (k_projection): ParametrizedLinear(\n",
       "    in_features=256, out_features=65536, bias=False\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): _WeightNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = encoder.mlp[0].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Constructs and initializes the classifier for fine-tuning and training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder: Encoder,\n",
    "        num_classes: int,\n",
    "        embedding_dim: int,\n",
    "        learning_rate: float,\n",
    "        eta_min: float,\n",
    "        weight_decay: float,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.eta_min = eta_min\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.encoder.requires_grad_(False)\n",
    "        self.encoder.eval()\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor\n",
    "        ):\n",
    "\n",
    "        h = self.encoder(x)\n",
    "        logits = self.fc(h)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def _pred_and_eval(self, batch):\n",
    "        img, target = batch\n",
    "        logits = self(img)\n",
    "\n",
    "        confidence = F.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(confidence, dim=1)\n",
    "\n",
    "        loss = self.criterion(logits, target)\n",
    "        accuracy = accuracy_score(target.cpu(), pred.cpu())\n",
    "\n",
    "        return loss, accuracy\n",
    "    \n",
    "    def training_step(self, batch, _):\n",
    "        train_loss, train_accuracy = self._pred_and_eval(batch)\n",
    "        optimizer = self.trainer.optimizers[0]\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        self.log(\"Train/Loss\", train_loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"Train/Accuracy\", train_accuracy, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"Learning Rate\", lr, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        metrics = {\n",
    "            \"loss\": train_loss,\n",
    "            \"accuracy\": train_accuracy\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def validation_step(self, batch, _):\n",
    "        val_loss, val_accuracy = self._pred_and_eval(batch)\n",
    "\n",
    "        self.log(\"Validation/Loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"Validation/Accuracy\", val_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        metrics = {\n",
    "            \"loss\": val_loss,\n",
    "            \"accuracy\": val_accuracy\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.encoder.parameters() + list(self.fc.parameters()))\n",
    "        optimizer = torch.optim.AdamW(params, lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self.trainer.max_epochs, eta_min=self.eta_min)\n",
    "\n",
    "        config = {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return config\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.3758,  0.4629, -2.6067,  ...,  0.4263,  1.6670, -0.4575],\n",
       "          [-1.0205, -0.2919,  0.9853,  ..., -0.8425, -0.5968,  0.2812],\n",
       "          [-1.3441, -0.4191,  1.2663,  ..., -0.6104, -1.3954, -0.3468],\n",
       "          ...,\n",
       "          [-0.1583,  0.6331,  1.2217,  ..., -1.0472,  0.0089, -0.1754],\n",
       "          [ 0.1801, -0.9542, -2.1773,  ..., -1.1632, -1.9616,  1.0455],\n",
       "          [-0.1797, -0.5522,  0.8437,  ...,  1.6244,  0.6465, -0.7188]],\n",
       "\n",
       "         [[-1.4782,  0.8769,  2.1025,  ...,  0.2194,  0.1642,  0.0179],\n",
       "          [ 2.1357,  0.3146, -0.2709,  ...,  2.5437, -0.4364,  0.5338],\n",
       "          [ 0.3186,  0.7456, -0.8078,  ...,  2.3906, -1.5178, -0.1774],\n",
       "          ...,\n",
       "          [ 0.0472, -0.6731,  1.4287,  ..., -0.6014,  0.7867,  0.0309],\n",
       "          [-0.7663, -0.1107, -0.1258,  ...,  0.9039,  0.3793, -0.1356],\n",
       "          [-0.5656,  2.4810,  0.1978,  ..., -1.3918,  0.9315,  1.1948]],\n",
       "\n",
       "         [[-0.0058, -1.5197,  2.4743,  ...,  0.3685,  1.8244,  0.2583],\n",
       "          [-2.6850,  0.0448,  1.0886,  ...,  0.3009,  2.0258, -1.1083],\n",
       "          [ 1.2069,  0.9495, -0.2357,  ..., -1.1599,  0.3771,  0.5566],\n",
       "          ...,\n",
       "          [-1.1322,  0.3490, -0.2286,  ...,  1.1723,  0.5707,  0.0261],\n",
       "          [ 0.6941,  1.2004,  1.6734,  ...,  1.5690, -2.2129, -0.0874],\n",
       "          [ 0.6326,  1.6406,  2.1627,  ...,  0.4843,  0.2263, -1.2858]]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.randn(1, 3, 224, 224)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Identity()\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = encoder.encoder\n",
    "\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = encoder(img)\n",
    "\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = nn.Linear(384, 10)\n",
    "\n",
    "logits = fc(h)\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4339, 0.0460, 0.0219, 0.0052, 0.0427, 0.0264, 0.0213, 0.3528, 0.0197,\n",
       "         0.0302]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
